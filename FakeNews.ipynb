{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLByLmAwZBSD",
        "outputId": "e5a82e4f-76e4-43e8-ed41-738da2df6810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.2.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=c46334fcc88ba2c9b0e3801e6d636437387578d1af7772e78f7e417781c4a839\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=b318a2428e24f93b5d557cb927898077be0ab2f568f361c7245c0d3e833e3bef\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=fa2de590d382cef1d8f2d2d81357b50173a92ab02dd360bbfc8cd2210830b717\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=8947e767ae4588c190503f765a95f137772f53a8b7d5867b2c6e15ae11a6c050\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.2.0\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.2)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install newspaper3k\n",
        "!pip install lxml_html_clean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from newspaper import Article\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import pipeline\n",
        "import urllib.request\n",
        "\n",
        "# -------- Train TF-IDF + Logistic Regression --------\n",
        "def train_model():\n",
        "    fake = pd.read_csv(\"/content/Fake.csv\")\n",
        "    true = pd.read_csv(\"/content/True.csv\")\n",
        "\n",
        "    fake[\"label\"], true[\"label\"] = 0, 1\n",
        "    df = pd.concat([fake, true])[[\"text\", \"label\"]].dropna()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    y = df[\"label\"]\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    return vectorizer, model\n",
        "\n",
        "vectorizer, tfidf_model = train_model()\n",
        "\n",
        "# -------- Transformers Zero-Shot Classifier --------\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# -------- Google Fact Check --------\n",
        "API_KEY = \"AIzaSyCrJzQ3Io7ji7xW_ermckT20XByvTlb63k\"\n",
        "\n",
        "def google_fact_check(query):\n",
        "    url = f\"https://factchecktools.googleapis.com/v1alpha1/claims:search?query={query}&key={API_KEY}\"\n",
        "    res = requests.get(url)\n",
        "    if res.status_code == 200:\n",
        "        claims = res.json().get(\"claims\", [])\n",
        "        for c in claims:\n",
        "            for review in c.get(\"claimReview\", []):\n",
        "                rating = review.get(\"textualRating\", \"\").lower()\n",
        "                if \"fake\" in rating or \"false\" in rating:\n",
        "                    return \"âŒ Verified Fake by Google Fact Check\"\n",
        "                elif \"true\" in rating:\n",
        "                    return \"âœ… Verified True by Google Fact Check\"\n",
        "    return None\n",
        "\n",
        "# -------- Article Scraper --------\n",
        "def scrape_article(url):\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        req = urllib.request.Request(url, headers=headers)\n",
        "        html = urllib.request.urlopen(req).read()\n",
        "\n",
        "        article = Article(url)\n",
        "        article.set_html(html)\n",
        "        article.parse()\n",
        "        return article.text\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# -------- Classify Text --------\n",
        "def classify_text(text, threshold=0.75):\n",
        "    result = classifier(text, candidate_labels=[\"True\", \"False\"])\n",
        "    true_score = result[\"scores\"][result[\"labels\"].index(\"True\")]\n",
        "\n",
        "    if true_score >= threshold:\n",
        "        return \"âœ… True News (via Transformer)\"\n",
        "    elif true_score <= (1 - threshold):\n",
        "        return \"âŒ Fake News (via Transformer)\"\n",
        "\n",
        "    # TF-IDF\n",
        "    pred = tfidf_model.predict(vectorizer.transform([text]))[0]\n",
        "    return \"âœ… True News (via TF-IDF)\" if pred == 1 else \"âŒ Fake News (via TF-IDF)\"\n",
        "\n",
        "# -------- Main Pipeline --------\n",
        "def check_news(text):\n",
        "    g_result = google_fact_check(text)\n",
        "    if g_result:\n",
        "        return g_result\n",
        "    return classify_text(text)\n",
        "\n",
        "# -------- Input & Output --------\n",
        "print(\"=\" * 60)\n",
        "print(\"ðŸ“° Fake News Detection Tool\")\n",
        "print(\"You can paste the news\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "user_input = input(\"ðŸ”¹ Enter text or URL: \").strip()\n",
        "\n",
        "if user_input.startswith(\"http\"):\n",
        "    print(\"\\nðŸŒ Extracting article content from URL...\\n\")\n",
        "    user_input = scrape_article(user_input)\n",
        "\n",
        "if user_input:\n",
        "    print(\"\\nðŸ”Ž Running authenticity checks...\\n\")\n",
        "    result = check_news(user_input)\n",
        "    print(\"ðŸ§¾ Final Verdict:\", result)\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"âš ï¸ No valid input provided. Please try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJk90tkzZS5b",
        "outputId": "0c700ae9-7c25-4a1a-81ab-ee199f711bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸ“° Fake News Detection Tool\n",
            "You can paste the news\n",
            "============================================================\n",
            "ðŸ”¹ Enter text or URL: https://economictimes.indiatimes.com/news/economy/foreign-trade/unplug-china-plug-into-india-centre-looks-to-attract-us-cos-planning-to-exit-china/articleshow/120356180.cms?from=mdr\n",
            "\n",
            "ðŸŒ Extracting article content from URL...\n",
            "\n",
            "\n",
            "ðŸ”Ž Running authenticity checks...\n",
            "\n",
            "ðŸ§¾ Final Verdict: âœ… True News (via Transformer)\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}